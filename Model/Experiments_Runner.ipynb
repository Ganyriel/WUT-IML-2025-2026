{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c24997d625edf12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T19:35:57.617226200Z",
     "start_time": "2025-12-14T19:35:54.307510500Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Add path to Helpers to import model_utils\n",
    "sys.path.append(os.path.abspath('../Helpers'))\n",
    "import model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3136a7e02d508f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T19:35:57.679178200Z",
     "start_time": "2025-12-14T19:35:57.618226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading manifest from: ../data_recordings\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "BASE_PATH = '../data_recordings'\n",
    "print(f\"Loading manifest from: {BASE_PATH}\")\n",
    "df = mu.load_manifest(BASE_PATH)\n",
    "\n",
    "# Optional: Sample data for quick debugging (uncomment next line)\n",
    "# df = df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d52fda978ca1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T19:37:04.990336400Z",
     "start_time": "2025-12-14T19:35:57.680177100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 11225 audio files...\n",
      "Successfully loaded: 11225, Failed: 0\n",
      "Generating Mel-Spectrograms...\n"
     ]
    }
   ],
   "source": [
    "# Load and process audio into spectrograms\n",
    "X, y, speakers = mu.load_and_process_data(df, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3803da8c2fc4edee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T19:37:05.626690200Z",
     "start_time": "2025-12-14T19:37:04.991336500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7859, 128, 130, 1)\n",
      "Val shape:   (1681, 128, 130, 1)\n",
      "Test shape:  (1685, 128, 130, 1)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Split Data ---\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, df_train, df_val, df_test = mu.split_dataset(X, y, speakers, df)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Val shape:   {X_val.shape}\")\n",
    "print(f\"Test shape:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7272101ccb35d1cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T19:37:05.636243300Z",
     "start_time": "2025-12-14T19:37:05.628691100Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Define Experiments ---\n",
    "# (Experiment Name, Optimizer, Dropout, Base LR, lr_schedule, weight_decay, use_plateau)\n",
    "experiments = [\n",
    "    # --- your existing ones (unchanged behavior) ---\n",
    "    (\"Baseline_Adam\",   \"adam\",  0.0, 0.001,  None,     0.0,   False),\n",
    "    (\"Baseline_SGD\",    \"sgd\",   0.0, 0.01,   None,     0.0,   False),\n",
    "    (\"Dropout_0.2\",     \"adam\",  0.2, 0.001,  None,     0.0,   False),\n",
    "    (\"Low_LR_Adam\",     \"adam\",  0.0, 0.0001, None,     0.0,   False),\n",
    "\n",
    "    # --- up to 4 new ones ---\n",
    "    (\"Adam_Plateau\",        \"adam\",  0.2, 0.001, None,     0.0,   True),\n",
    "    (\"SGD_Plateau\",         \"sgd\",   0.0, 0.01,  None,     0.0,   True),\n",
    "    (\"AdamW_Cosine\",        \"adamw\", 0.2, 0.001, \"cosine\",  1e-4,  False),\n",
    "    (\"AdamW_Plateau\",       \"adamw\", 0.2, 0.001, None,     1e-4,  True),\n",
    "]\n",
    "results_summary = []\n",
    "# Create a directory for results if it doesn't exist\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c443a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n",
      "TensorFlow: 2.20.0\n",
      "keras package: 3.12.0\n",
      "keras module file: c:\\Users\\adamf\\Desktop\\student_debil\\semestr7\\WUT-IML-2025-2026\\.venv\\Lib\\site-packages\\keras\\__init__.py\n",
      "tf.keras module: tensorflow.keras\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"keras package:\", keras.__version__)\n",
    "print(\"keras module file:\", keras.__file__)\n",
    "print(\"tf.keras module:\", tf.keras.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f585ff885d5b8a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:18:41.506545100Z",
     "start_time": "2025-12-14T19:37:05.637244700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running Experiment: Baseline_Adam\n",
      "Params: Opt=adam, Dropout=0.0, LR=0.001, Sched=None, WD=0.0, Plateau=False\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 691ms/step - accuracy: 0.6712 - loss: 0.5852 - val_accuracy: 0.7537 - val_loss: 0.4675\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 653ms/step - accuracy: 0.8527 - loss: 0.3348 - val_accuracy: 0.8828 - val_loss: 0.2870\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 666ms/step - accuracy: 0.8963 - loss: 0.2452 - val_accuracy: 0.9078 - val_loss: 0.2441\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 646ms/step - accuracy: 0.9197 - loss: 0.1937 - val_accuracy: 0.8626 - val_loss: 0.3448\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 639ms/step - accuracy: 0.9357 - loss: 0.1526 - val_accuracy: 0.9072 - val_loss: 0.2456\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 654ms/step - accuracy: 0.9466 - loss: 0.1356 - val_accuracy: 0.9227 - val_loss: 0.2059\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 679ms/step - accuracy: 0.9597 - loss: 0.1066 - val_accuracy: 0.9262 - val_loss: 0.2147\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 646ms/step - accuracy: 0.9691 - loss: 0.0802 - val_accuracy: 0.9334 - val_loss: 0.2040\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 642ms/step - accuracy: 0.9748 - loss: 0.0660 - val_accuracy: 0.9155 - val_loss: 0.2605\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 647ms/step - accuracy: 0.9804 - loss: 0.0503 - val_accuracy: 0.9429 - val_loss: 0.1829\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 645ms/step - accuracy: 0.9836 - loss: 0.0406 - val_accuracy: 0.9352 - val_loss: 0.1988\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 647ms/step - accuracy: 0.9860 - loss: 0.0371 - val_accuracy: 0.9369 - val_loss: 0.2254\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 649ms/step - accuracy: 0.9861 - loss: 0.0383 - val_accuracy: 0.9381 - val_loss: 0.2257\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 809ms/step - accuracy: 0.9902 - loss: 0.0278 - val_accuracy: 0.9363 - val_loss: 0.2060\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 1s/step - accuracy: 0.9931 - loss: 0.0200 - val_accuracy: 0.9334 - val_loss: 0.2602\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Test Accuracy: 0.9448\n",
      "Model saved to results/model_Baseline_Adam.keras\n",
      "Generating predictions for analysis...\n",
      "\n",
      "========================================\n",
      "Running Experiment: Baseline_SGD\n",
      "Params: Opt=sgd, Dropout=0.0, LR=0.01, Sched=None, WD=0.0, Plateau=False\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 549ms/step - accuracy: 0.6007 - loss: 0.6550 - val_accuracy: 0.6901 - val_loss: 0.5730\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 580ms/step - accuracy: 0.7777 - loss: 0.4619 - val_accuracy: 0.7406 - val_loss: 0.4921\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 636ms/step - accuracy: 0.8754 - loss: 0.2948 - val_accuracy: 0.9185 - val_loss: 0.2197\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 641ms/step - accuracy: 0.9083 - loss: 0.2179 - val_accuracy: 0.9155 - val_loss: 0.2059\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 607ms/step - accuracy: 0.9398 - loss: 0.1516 - val_accuracy: 0.9060 - val_loss: 0.2450\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 589ms/step - accuracy: 0.9585 - loss: 0.1088 - val_accuracy: 0.9304 - val_loss: 0.1804\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 558ms/step - accuracy: 0.9690 - loss: 0.0823 - val_accuracy: 0.9304 - val_loss: 0.2099\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 596ms/step - accuracy: 0.9805 - loss: 0.0508 - val_accuracy: 0.9566 - val_loss: 0.1261\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 640ms/step - accuracy: 0.9863 - loss: 0.0418 - val_accuracy: 0.9584 - val_loss: 0.1400\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 568ms/step - accuracy: 0.9805 - loss: 0.0489 - val_accuracy: 0.9369 - val_loss: 0.1536\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 499ms/step - accuracy: 0.9874 - loss: 0.0323 - val_accuracy: 0.9268 - val_loss: 0.2561\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 543ms/step - accuracy: 0.9943 - loss: 0.0185 - val_accuracy: 0.9697 - val_loss: 0.0975\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 497ms/step - accuracy: 0.9957 - loss: 0.0121 - val_accuracy: 0.9447 - val_loss: 0.2175\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 484ms/step - accuracy: 0.9920 - loss: 0.0206 - val_accuracy: 0.9512 - val_loss: 0.1573\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 488ms/step - accuracy: 0.9882 - loss: 0.0346 - val_accuracy: 0.9536 - val_loss: 0.1709\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Test Accuracy: 0.9549\n",
      "Model saved to results/model_Baseline_SGD.keras\n",
      "Generating predictions for analysis...\n",
      "\n",
      "========================================\n",
      "Running Experiment: Dropout_0.2\n",
      "Params: Opt=adam, Dropout=0.2, LR=0.001, Sched=None, WD=0.0, Plateau=False\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 562ms/step - accuracy: 0.6016 - loss: 0.6672 - val_accuracy: 0.7103 - val_loss: 0.5447\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 568ms/step - accuracy: 0.7689 - loss: 0.4744 - val_accuracy: 0.8471 - val_loss: 0.3504\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 571ms/step - accuracy: 0.8387 - loss: 0.3654 - val_accuracy: 0.8941 - val_loss: 0.2845\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 566ms/step - accuracy: 0.8731 - loss: 0.2888 - val_accuracy: 0.8971 - val_loss: 0.2538\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 561ms/step - accuracy: 0.9070 - loss: 0.2350 - val_accuracy: 0.9328 - val_loss: 0.1866\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 562ms/step - accuracy: 0.9254 - loss: 0.1865 - val_accuracy: 0.9494 - val_loss: 0.1361\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 562ms/step - accuracy: 0.9387 - loss: 0.1607 - val_accuracy: 0.9512 - val_loss: 0.1301\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 558ms/step - accuracy: 0.9411 - loss: 0.1496 - val_accuracy: 0.9500 - val_loss: 0.1227\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 565ms/step - accuracy: 0.9520 - loss: 0.1286 - val_accuracy: 0.9500 - val_loss: 0.1369\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 562ms/step - accuracy: 0.9561 - loss: 0.1159 - val_accuracy: 0.9584 - val_loss: 0.1104\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 562ms/step - accuracy: 0.9641 - loss: 0.1007 - val_accuracy: 0.9625 - val_loss: 0.1099\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 618ms/step - accuracy: 0.9630 - loss: 0.1024 - val_accuracy: 0.9542 - val_loss: 0.1140\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 608ms/step - accuracy: 0.9676 - loss: 0.0836 - val_accuracy: 0.9625 - val_loss: 0.1112\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 704ms/step - accuracy: 0.9701 - loss: 0.0860 - val_accuracy: 0.9619 - val_loss: 0.1019\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 601ms/step - accuracy: 0.9733 - loss: 0.0710 - val_accuracy: 0.9667 - val_loss: 0.0955\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Test Accuracy: 0.9596\n",
      "Model saved to results/model_Dropout_0.2.keras\n",
      "Generating predictions for analysis...\n",
      "Generating Monte Carlo Dropout predictions...\n",
      "\n",
      "--- MC Dropout Uncertainty Summary ---\n",
      "Mean uncertainty (std): 0.0676\n",
      "Median uncertainty:     0.0161\n",
      "Max uncertainty:        0.4336\n",
      "\n",
      "--- Uncertainty vs Correctness ---\n",
      "Mean uncertainty (correct preds): 0.0599\n",
      "Mean uncertainty (wrong preds):   0.2535\n",
      "\n",
      "========================================\n",
      "Running Experiment: Low_LR_Adam\n",
      "Params: Opt=adam, Dropout=0.0, LR=0.0001, Sched=None, WD=0.0, Plateau=False\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 927ms/step - accuracy: 0.6501 - loss: 0.6148 - val_accuracy: 0.7496 - val_loss: 0.4964\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 492ms/step - accuracy: 0.8193 - loss: 0.4013 - val_accuracy: 0.8543 - val_loss: 0.3439\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 498ms/step - accuracy: 0.8703 - loss: 0.3033 - val_accuracy: 0.8816 - val_loss: 0.2996\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 508ms/step - accuracy: 0.8972 - loss: 0.2524 - val_accuracy: 0.8798 - val_loss: 0.2780\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 514ms/step - accuracy: 0.9085 - loss: 0.2196 - val_accuracy: 0.8840 - val_loss: 0.2795\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 672ms/step - accuracy: 0.9210 - loss: 0.1981 - val_accuracy: 0.9048 - val_loss: 0.2436\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 532ms/step - accuracy: 0.9245 - loss: 0.1783 - val_accuracy: 0.9096 - val_loss: 0.2364\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 548ms/step - accuracy: 0.9380 - loss: 0.1534 - val_accuracy: 0.9048 - val_loss: 0.2412\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 536ms/step - accuracy: 0.9491 - loss: 0.1364 - val_accuracy: 0.9185 - val_loss: 0.1979\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 572ms/step - accuracy: 0.9511 - loss: 0.1252 - val_accuracy: 0.9221 - val_loss: 0.1959\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 601ms/step - accuracy: 0.9597 - loss: 0.1094 - val_accuracy: 0.9274 - val_loss: 0.1798\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 626ms/step - accuracy: 0.9665 - loss: 0.0937 - val_accuracy: 0.9268 - val_loss: 0.1848\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 693ms/step - accuracy: 0.9695 - loss: 0.0865 - val_accuracy: 0.8769 - val_loss: 0.3464\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 604ms/step - accuracy: 0.9693 - loss: 0.0835 - val_accuracy: 0.9310 - val_loss: 0.1821\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 870ms/step - accuracy: 0.9804 - loss: 0.0628 - val_accuracy: 0.9209 - val_loss: 0.2070\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Test Accuracy: 0.9252\n",
      "Model saved to results/model_Low_LR_Adam.keras\n",
      "Generating predictions for analysis...\n",
      "\n",
      "========================================\n",
      "Running Experiment: Adam_Plateau\n",
      "Params: Opt=adam, Dropout=0.2, LR=0.001, Sched=None, WD=0.0, Plateau=True\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 788ms/step - accuracy: 0.6043 - loss: 0.6583 - val_accuracy: 0.7002 - val_loss: 0.5532 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 784ms/step - accuracy: 0.7693 - loss: 0.4827 - val_accuracy: 0.8543 - val_loss: 0.3717 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 778ms/step - accuracy: 0.8341 - loss: 0.3705 - val_accuracy: 0.9018 - val_loss: 0.2887 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 764ms/step - accuracy: 0.8703 - loss: 0.3019 - val_accuracy: 0.8905 - val_loss: 0.2653 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 757ms/step - accuracy: 0.9013 - loss: 0.2393 - val_accuracy: 0.9310 - val_loss: 0.1904 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 751ms/step - accuracy: 0.9193 - loss: 0.2039 - val_accuracy: 0.9352 - val_loss: 0.1659 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 739ms/step - accuracy: 0.9359 - loss: 0.1683 - val_accuracy: 0.9447 - val_loss: 0.1429 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 750ms/step - accuracy: 0.9415 - loss: 0.1499 - val_accuracy: 0.9411 - val_loss: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 781ms/step - accuracy: 0.9459 - loss: 0.1403 - val_accuracy: 0.9560 - val_loss: 0.1126 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 755ms/step - accuracy: 0.9570 - loss: 0.1175 - val_accuracy: 0.9578 - val_loss: 0.1218 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725ms/step - accuracy: 0.9562 - loss: 0.1153\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 753ms/step - accuracy: 0.9603 - loss: 0.1065 - val_accuracy: 0.9530 - val_loss: 0.1266 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 744ms/step - accuracy: 0.9711 - loss: 0.0851 - val_accuracy: 0.9584 - val_loss: 0.1193 - learning_rate: 5.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 761ms/step - accuracy: 0.9747 - loss: 0.0722 - val_accuracy: 0.9601 - val_loss: 0.1050 - learning_rate: 5.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 879ms/step - accuracy: 0.9749 - loss: 0.0675 - val_accuracy: 0.9637 - val_loss: 0.1001 - learning_rate: 5.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 819ms/step - accuracy: 0.9768 - loss: 0.0665 - val_accuracy: 0.9673 - val_loss: 0.1048 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Test Accuracy: 0.9573\n",
      "Model saved to results/model_Adam_Plateau.keras\n",
      "Generating predictions for analysis...\n",
      "Generating Monte Carlo Dropout predictions...\n",
      "\n",
      "--- MC Dropout Uncertainty Summary ---\n",
      "Mean uncertainty (std): 0.0687\n",
      "Median uncertainty:     0.0154\n",
      "Max uncertainty:        0.4184\n",
      "\n",
      "--- Uncertainty vs Correctness ---\n",
      "Mean uncertainty (correct preds): 0.0607\n",
      "Mean uncertainty (wrong preds):   0.2514\n",
      "\n",
      "========================================\n",
      "Running Experiment: SGD_Plateau\n",
      "Params: Opt=sgd, Dropout=0.0, LR=0.01, Sched=None, WD=0.0, Plateau=True\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 913ms/step - accuracy: 0.5980 - loss: 0.6539 - val_accuracy: 0.7686 - val_loss: 0.5439 - learning_rate: 0.0100\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 522ms/step - accuracy: 0.7610 - loss: 0.4829 - val_accuracy: 0.8507 - val_loss: 0.3660 - learning_rate: 0.0100\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 521ms/step - accuracy: 0.8487 - loss: 0.3396 - val_accuracy: 0.9012 - val_loss: 0.2545 - learning_rate: 0.0100\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 550ms/step - accuracy: 0.9047 - loss: 0.2261 - val_accuracy: 0.8959 - val_loss: 0.2420 - learning_rate: 0.0100\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 638ms/step - accuracy: 0.9240 - loss: 0.1797 - val_accuracy: 0.9298 - val_loss: 0.1839 - learning_rate: 0.0100\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 499ms/step - accuracy: 0.9483 - loss: 0.1305 - val_accuracy: 0.9215 - val_loss: 0.2046 - learning_rate: 0.0100\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 537ms/step - accuracy: 0.9597 - loss: 0.1071 - val_accuracy: 0.9399 - val_loss: 0.1644 - learning_rate: 0.0100\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 521ms/step - accuracy: 0.9723 - loss: 0.0747 - val_accuracy: 0.9435 - val_loss: 0.1403 - learning_rate: 0.0100\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 535ms/step - accuracy: 0.9808 - loss: 0.0490 - val_accuracy: 0.9542 - val_loss: 0.1307 - learning_rate: 0.0100\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 520ms/step - accuracy: 0.9781 - loss: 0.0552 - val_accuracy: 0.9524 - val_loss: 0.1518 - learning_rate: 0.0100\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507ms/step - accuracy: 0.9922 - loss: 0.0260\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 533ms/step - accuracy: 0.9889 - loss: 0.0340 - val_accuracy: 0.9482 - val_loss: 0.1397 - learning_rate: 0.0100\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 525ms/step - accuracy: 0.9966 - loss: 0.0127 - val_accuracy: 0.9637 - val_loss: 0.1340 - learning_rate: 0.0050\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 522ms/step - accuracy: 0.9995 - loss: 0.0033 - val_accuracy: 0.9673 - val_loss: 0.1294 - learning_rate: 0.0050\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 522ms/step - accuracy: 0.9999 - loss: 0.0020 - val_accuracy: 0.9667 - val_loss: 0.1305 - learning_rate: 0.0050\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.9994 - loss: 0.0018\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 523ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9691 - val_loss: 0.1484 - learning_rate: 0.0050\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Test Accuracy: 0.9585\n",
      "Model saved to results/model_SGD_Plateau.keras\n",
      "Generating predictions for analysis...\n",
      "\n",
      "========================================\n",
      "Running Experiment: AdamW_Cosine\n",
      "Params: Opt=adamw, Dropout=0.2, LR=0.001, Sched=cosine, WD=0.0001, Plateau=False\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 621ms/step - accuracy: 0.5436 - loss: 0.6984 - val_accuracy: 0.5628 - val_loss: 0.6803\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 633ms/step - accuracy: 0.7198 - loss: 0.5531 - val_accuracy: 0.7942 - val_loss: 0.4746\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 621ms/step - accuracy: 0.8180 - loss: 0.3998 - val_accuracy: 0.8543 - val_loss: 0.3325\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 665ms/step - accuracy: 0.8590 - loss: 0.3192 - val_accuracy: 0.8935 - val_loss: 0.2714\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 628ms/step - accuracy: 0.8876 - loss: 0.2704 - val_accuracy: 0.9030 - val_loss: 0.2259\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 638ms/step - accuracy: 0.9111 - loss: 0.2201 - val_accuracy: 0.9328 - val_loss: 0.1780\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 622ms/step - accuracy: 0.9181 - loss: 0.2003 - val_accuracy: 0.9352 - val_loss: 0.1643\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 626ms/step - accuracy: 0.9328 - loss: 0.1722 - val_accuracy: 0.9494 - val_loss: 0.1310\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 647ms/step - accuracy: 0.9408 - loss: 0.1546 - val_accuracy: 0.9494 - val_loss: 0.1244\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 624ms/step - accuracy: 0.9490 - loss: 0.1284 - val_accuracy: 0.9595 - val_loss: 0.1124\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 639ms/step - accuracy: 0.9602 - loss: 0.1150 - val_accuracy: 0.9554 - val_loss: 0.1150\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 635ms/step - accuracy: 0.9567 - loss: 0.1155 - val_accuracy: 0.9572 - val_loss: 0.1146\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 633ms/step - accuracy: 0.9584 - loss: 0.1094 - val_accuracy: 0.9613 - val_loss: 0.1096\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 568ms/step - accuracy: 0.9631 - loss: 0.1038 - val_accuracy: 0.9584 - val_loss: 0.1092\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 561ms/step - accuracy: 0.9654 - loss: 0.0988 - val_accuracy: 0.9584 - val_loss: 0.1089\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Test Accuracy: 0.9466\n",
      "Model saved to results/model_AdamW_Cosine.keras\n",
      "Generating predictions for analysis...\n",
      "Generating Monte Carlo Dropout predictions...\n",
      "\n",
      "--- MC Dropout Uncertainty Summary ---\n",
      "Mean uncertainty (std): 0.0850\n",
      "Median uncertainty:     0.0438\n",
      "Max uncertainty:        0.4138\n",
      "\n",
      "--- Uncertainty vs Correctness ---\n",
      "Mean uncertainty (correct preds): 0.0755\n",
      "Mean uncertainty (wrong preds):   0.2425\n",
      "\n",
      "========================================\n",
      "Running Experiment: AdamW_Plateau\n",
      "Params: Opt=adamw, Dropout=0.2, LR=0.001, Sched=None, WD=0.0001, Plateau=True\n",
      "========================================\n",
      "Epoch 1/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 1s/step - accuracy: 0.5759 - loss: 0.6890 - val_accuracy: 0.6853 - val_loss: 0.6270 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 602ms/step - accuracy: 0.7264 - loss: 0.5347 - val_accuracy: 0.7995 - val_loss: 0.4091 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 585ms/step - accuracy: 0.8226 - loss: 0.3826 - val_accuracy: 0.8763 - val_loss: 0.2810 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 592ms/step - accuracy: 0.8700 - loss: 0.2988 - val_accuracy: 0.8953 - val_loss: 0.2407 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 597ms/step - accuracy: 0.9053 - loss: 0.2338 - val_accuracy: 0.9363 - val_loss: 0.1721 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 580ms/step - accuracy: 0.9284 - loss: 0.1836 - val_accuracy: 0.9435 - val_loss: 0.1551 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 597ms/step - accuracy: 0.9426 - loss: 0.1517 - val_accuracy: 0.9471 - val_loss: 0.1386 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 599ms/step - accuracy: 0.9502 - loss: 0.1308 - val_accuracy: 0.9566 - val_loss: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 596ms/step - accuracy: 0.9499 - loss: 0.1242 - val_accuracy: 0.9453 - val_loss: 0.1325 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 589ms/step - accuracy: 0.9565 - loss: 0.1093 - val_accuracy: 0.9607 - val_loss: 0.1040 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 597ms/step - accuracy: 0.9660 - loss: 0.0895 - val_accuracy: 0.9572 - val_loss: 0.1071 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 586ms/step - accuracy: 0.9663 - loss: 0.0924 - val_accuracy: 0.9673 - val_loss: 0.0848 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 617ms/step - accuracy: 0.9664 - loss: 0.0866 - val_accuracy: 0.9607 - val_loss: 0.1133 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556ms/step - accuracy: 0.9671 - loss: 0.0866\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 580ms/step - accuracy: 0.9679 - loss: 0.0843 - val_accuracy: 0.9578 - val_loss: 0.1104 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m246/246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 598ms/step - accuracy: 0.9800 - loss: 0.0551 - val_accuracy: 0.9691 - val_loss: 0.0900 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Test Accuracy: 0.9644\n",
      "Model saved to results/model_AdamW_Plateau.keras\n",
      "Generating predictions for analysis...\n",
      "Generating Monte Carlo Dropout predictions...\n",
      "\n",
      "--- MC Dropout Uncertainty Summary ---\n",
      "Mean uncertainty (std): 0.0714\n",
      "Median uncertainty:     0.0238\n",
      "Max uncertainty:        0.4193\n",
      "\n",
      "--- Uncertainty vs Correctness ---\n",
      "Mean uncertainty (correct preds): 0.0653\n",
      "Mean uncertainty (wrong preds):   0.2360\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Run Experiments Loop ---\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "steps_per_epoch = int(tf.math.ceil(len(X_train) / batch_size))\n",
    "\n",
    "for exp_name, opt, drop, lr, lr_sched, wd, use_plateau in experiments:\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"Running Experiment: {exp_name}\")\n",
    "    print(f\"Params: Opt={opt}, Dropout={drop}, LR={lr}, Sched={lr_sched}, WD={wd}, Plateau={use_plateau}\")\n",
    "    print(\"=\"*40)\n",
    "    model = mu.build_model(\n",
    "        input_shape=X_train.shape[1:],\n",
    "        optimizer_name=opt,\n",
    "        dropout_rate=drop,\n",
    "        learning_rate=lr,\n",
    "        lr_schedule=lr_sched,\n",
    "        weight_decay=wd,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        total_epochs=epochs,\n",
    "        adapt_data=X_train\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.CSVLogger(f\"results/logs_{exp_name}.csv\"),\n",
    "        *mu.make_lr_callbacks(use_plateau=use_plateau),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate on Test Set\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Record results\n",
    "    results_summary.append({\n",
    "        \"Experiment\": exp_name,\n",
    "        \"Test_Accuracy\": acc,\n",
    "        \"Test_Loss\": loss,\n",
    "        \"Optimizer\": opt,\n",
    "        \"Dropout\": drop,\n",
    "        \"Learning_Rate\": lr\n",
    "    })\n",
    "\n",
    "    # Save Model\n",
    "    model_save_path = f\"results/model_{exp_name}.keras\"\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    # --- IMPORTANT FOR EDA ---\n",
    "    # Save predictions with metadata for Error Analysis\n",
    "    print(\"Generating predictions for analysis...\")\n",
    "    preds_prob = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "    # Create DataFrame with true labels, predictions, AND metadata (filenames)\n",
    "    # We use df_test which we got from the split function\n",
    "    pred_df = df_test.copy()\n",
    "    pred_df[\"true_label\"] = y_test\n",
    "    pred_df[\"pred_prob\"] = preds_prob\n",
    "    pred_df[\"pred_label\"] = (preds_prob > 0.5).astype(int)\n",
    "\n",
    "    # Save to CSV\n",
    "    pred_df.to_csv(f\"results/predictions_{exp_name}.csv\", index=False)\n",
    "\n",
    "    if drop > 0:\n",
    "        print(\"Generating Monte Carlo Dropout predictions...\")\n",
    "        mc_mean, mc_std = mu.mc_dropout_predict(model, X_test, n_passes=15)\n",
    "\n",
    "        mc_df = df_test.copy()\n",
    "        mc_df[\"true_label\"] = y_test\n",
    "        mc_df[\"pred_prob_mean\"] = mc_mean\n",
    "        mc_df[\"pred_prob_std\"] = mc_std\n",
    "        mc_df[\"pred_label\"] = (mc_mean > 0.5).astype(int)\n",
    "\n",
    "        print(\"\\n--- MC Dropout Uncertainty Summary ---\")\n",
    "        print(f\"Mean uncertainty (std): {mc_std.mean():.4f}\")\n",
    "        print(f\"Median uncertainty:     {np.median(mc_std):.4f}\")\n",
    "        print(f\"Max uncertainty:        {mc_std.max():.4f}\")\n",
    "\n",
    "        correct_mask = (mc_df[\"pred_label\"].values == y_test)\n",
    "\n",
    "        mean_unc_correct = mc_std[correct_mask].mean()\n",
    "        mean_unc_wrong   = mc_std[~correct_mask].mean()\n",
    "\n",
    "        print(\"\\n--- Uncertainty vs Correctness ---\")\n",
    "        print(f\"Mean uncertainty (correct preds): {mean_unc_correct:.4f}\")\n",
    "        print(f\"Mean uncertainty (wrong preds):   {mean_unc_wrong:.4f}\")\n",
    "\n",
    "\n",
    "        mc_df.to_csv(f\"results/predictions_mc_{exp_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T20:18:41.540320600Z",
     "start_time": "2025-12-14T20:18:41.516320900Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Experiments Summary ===\n",
      "      Experiment  Test_Accuracy  Test_Loss Optimizer  Dropout  Learning_Rate  \\\n",
      "0  Baseline_Adam       0.944807   0.157262      adam      0.0         0.0010   \n",
      "1   Baseline_SGD       0.954896   0.144329       sgd      0.0         0.0100   \n",
      "2    Dropout_0.2       0.959644   0.106261      adam      0.2         0.0010   \n",
      "3    Low_LR_Adam       0.925223   0.183353      adam      0.0         0.0001   \n",
      "4   Adam_Plateau       0.957270   0.115072      adam      0.2         0.0010   \n",
      "5    SGD_Plateau       0.958457   0.149510       sgd      0.0         0.0100   \n",
      "6   AdamW_Cosine       0.946588   0.126059     adamw      0.2         0.0010   \n",
      "7  AdamW_Plateau       0.964392   0.102901     adamw      0.2         0.0010   \n",
      "\n",
      "             timestamp  \n",
      "0  2025-12-17 21:41:06  \n",
      "1  2025-12-17 21:41:06  \n",
      "2  2025-12-17 21:41:06  \n",
      "3  2025-12-17 21:41:06  \n",
      "4  2025-12-17 21:41:06  \n",
      "5  2025-12-17 21:41:06  \n",
      "6  2025-12-17 21:41:06  \n",
      "7  2025-12-17 21:41:06  \n",
      "\n",
      "All experiments completed.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Summary ---\n",
    "summary_df = pd.DataFrame(results_summary)\n",
    "\n",
    "# Add a timestamp column with the current date and time\n",
    "summary_df['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(\"\\n=== Final Experiments Summary ===\")\n",
    "print(summary_df)\n",
    "\n",
    "summary_path = \"results/experiments_summary.csv\"\n",
    "\n",
    "# Check if file exists to determine whether to write the header\n",
    "# Note: If an old file exists without a 'timestamp' column, appending might cause\n",
    "# structure issues. It is best to delete the old file or manually add a column header to it.\n",
    "write_header = not os.path.exists(summary_path)\n",
    "\n",
    "# Save to CSV with mode='a' (append)\n",
    "summary_df.to_csv(summary_path, mode='a', header=write_header, index=False)\n",
    "\n",
    "print(\"\\nAll experiments completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
